import os
import pickle
from langchain_community.vectorstores import FAISS # ê²€ìƒ‰ìš© ë²¡í„° ì €ì¥ì†Œ (local ë²¡í„° DB) 
from langchain.schema import Document # LangChainì—ì„œ ë¬¸ì„œë¥¼ ë‹´ëŠ” ê°ì²´
from langchain.text_splitter import RecursiveCharacterTextSplitter  # ë¬¸ì„œë¥¼ ì—¬ëŸ¬ ë¸”ë¡ìœ¼ë¡œ ìë¦„
from langchain_huggingface import HuggingFaceEmbeddings # ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜í•  ì„ë² ë”© ëª¨ë¸
from dotenv import load_dotenv


# .envì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
os.environ["HUGGINGFACEHUB_API_TOKEN"] = os.getenv("HUGGINGFACEHUB_API_TOKEN")


# embedding = HuggingFaceEmbeddings(
#     model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
# )

# ì„ë² ë”© ëª¨ë¸ 
embedding = HuggingFaceEmbeddings(
    model_name="jhgan/ko-sroberta-multitask"
)


BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # back/
VECTORSTORE_PATH = os.path.join(BASE_DIR, "vectorstore", "faiss_store.pkl")
DOCS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "documents")
# ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì €ì¥í•  íŒŒì¼ëª…ì„ ì§€ì •í•´ë†“ìŒ
#   DOC_PATH = os.path.join(os.path.dirname(__file__), "documents", "íœ´ë¨¼_2êµìœ¡ì‹¤_ë‚´ë¶€ë°ì´í„°.txt")


# ë‚´ë¶€ ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸° 
def load_documents():
    DOCS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "documents")
    documents = []

    for filename in os.listdir(DOCS_DIR):
        if filename.endswith(".txt"):
            filepath = os.path.join(DOCS_DIR, filename)
            with open(filepath, "r", encoding="utf-8") as f:
                text = f.read()
                documents.append(Document(page_content=text, metadata={"source": filename}))

    if not documents:
        raise FileNotFoundError(f"ğŸ“‚ documents í´ë”ì— .txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {DOCS_DIR}")
    
    return documents


def save_vectorstore(vectorstore, path=VECTORSTORE_PATH):
    with open(path, "wb") as f:
        pickle.dump(vectorstore, f)

# def load_vectorstore():
#     docs = load_documents()
#     splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30) # ë¬¸ì„œ ìª¼ê°œê³  ê²¹ì¹˜ë„ë¡ ë‚˜ëˆ” 
#     splits = splitter.split_documents(docs)
#     return FAISS.from_documents(splits, embedding) 
#     # ê°ê°ì˜ split ë¬¸ì„œë¥¼ embedding ëª¨ë¸ë¡œ ìµœì í™” , ë²¡í„°ë¥¼ FAISS ì— ì €ì¥ í›„ vectorstore ê°ì²´ ë°˜í™˜ 

def load_vectorstore():
    if os.path.exists(VECTORSTORE_PATH): # ì´ë¯¸ ë§Œë“¤ì–´ë‘” FAISS ë²¡í„° DB ê°€ ìˆìœ¼ë©´ ë¡œë“œí•´ì„œ ì¬ì‚¬ìš© 
        with open(VECTORSTORE_PATH, "rb") as f:
            print("ìºì‹œëœ vectorstore ë¡œë“œë¨")
            return pickle.load(f)
    else:
        print("vectorstore ìƒˆë¡œ ìƒì„± ì¤‘...")
        docs = load_documents()
        splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)
        splits = splitter.split_documents(docs)
        vectorstore = FAISS.from_documents(splits, embedding)
        save_vectorstore(vectorstore)
        return vectorstore

def generate_rag_answer(model, question, vectorstore, k=4): 
    # ì§ˆë¬¸ì— ëŒ€í•´ ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ë¹„ìŠ·í•œ ë¬¸ì„œ kê°œ ê²€ìƒ‰ 
    docs = vectorstore.similarity_search(question, k=k) 
    context = "\n".join([f"ë¬¸ì„œ{i+1}: {doc.page_content}" for i, doc in enumerate(docs)])
    prompt = f"""Contextì— ê¸°ë°˜í•´ì„œ ì§§ê²Œ ëŒ€ë‹µí•´ì¤˜.
    Context:
    {context}
    ì§ˆë¬¸: {question}
    ëŒ€ë‹µ:"""
    response = model.generate_content(prompt)
    answer = response.candidates[0].content.parts[0].text
    return answer, context
