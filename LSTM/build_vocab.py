# build_vocab.py

import re
from collections import Counter
import pickle
import torch

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ + í† í°í™” í•¨ìˆ˜
def tokenizer(text):
    text = re.sub(r"[^ê°€-í£0-9\s]", "", text)
    return text.strip().split()

# vocab ìƒì„± í•¨ìˆ˜
def build_vocab(file_path, vocab_size=10000):
    counter = Counter()
    with open(file_path, 'r', encoding='utf-8') as f:
        next(f)  # ì²« ì¤„ skip
        for line in f:
            try:
                _, text, _ = line.strip().split('\t')
                tokens = tokenizer(text)
                counter.update(tokens)
            except:
                continue

    most_common = counter.most_common(vocab_size - 2)
    vocab = {'<pad>': 0, '<unk>': 1}
    for i, (word, _) in enumerate(most_common, start=2):
        vocab[word] = i

    return vocab

# ğŸ”§ vocab ìƒì„± ë° ì €ì¥
if __name__ == "__main__":
    vocab = build_vocab("data-files/ratings_train.txt")
    with open("vocab.pkl", "wb") as f:
        pickle.dump(vocab, f)
    print("âœ… vocab.pkl ìƒì„± ì™„ë£Œ!")


# ğŸ“¦ Flaskì—ì„œ ì‚¬ìš©í•  ì „ì²˜ë¦¬ í•¨ìˆ˜ 
def preprocess_input(text, vocab, max_len=100):
    tokens = tokenizer(text)
    ids = [vocab.get(token, vocab['<unk>']) for token in tokens]
    if len(ids) < max_len:
        ids += [vocab['<pad>']] * (max_len - len(ids))
    else:
        ids = ids[:max_len]
    return torch.tensor([ids], dtype=torch.long)

