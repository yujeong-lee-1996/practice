import os
import pickle
from langchain_community.vectorstores import FAISS # ê²€ìƒ‰ìš© ë²¡í„° ì €ì¥ì†Œ (local ë²¡í„° DB) 
from langchain.schema import Document # LangChainì—ì„œ ë¬¸ì„œë¥¼ ë‹´ëŠ” ê°ì²´
from langchain.text_splitter import RecursiveCharacterTextSplitter  # ë¬¸ì„œë¥¼ ì—¬ëŸ¬ ë¸”ë¡ìœ¼ë¡œ ìë¦„
from langchain_huggingface import HuggingFaceEmbeddings # ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜í•  ì„ë² ë”© ëª¨ë¸
from dotenv import load_dotenv
from keybert import KeyBERT


# í™˜ê²½ ë³€ìˆ˜ ë¡œë”© ë° HuggingFace API ì„¤ì •
load_dotenv()
os.environ["HUGGINGFACEHUB_API_TOKEN"] = os.getenv("HUGGINGFACEHUB_API_TOKEN")

kw_model = KeyBERT(model="paraphrase-MiniLM-L6-v2")

# embedding = HuggingFaceEmbeddings(
#     model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
# )

# ì„ë² ë”© ëª¨ë¸ ì„¤ì • 
embedding = HuggingFaceEmbeddings(
    model_name="jhgan/ko-sroberta-multitask"
)

# ê²½ë¡œ ì„¤ì • 
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # back/
VECTORSTORE_PATH = os.path.join(BASE_DIR, "vectorstore", "faiss_store.pkl") # ë²¡í„°DB ìºì‹œ ì €ì¥ ìœ„ì¹˜ 
DOCS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "documents") # documents/*.txt: í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ì½ì–´ì˜¬ ë””ë ‰í† ë¦¬



# ë‚´ë¶€ ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸° 
def load_documents():
    DOCS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "documents")
    documents = []

    for filename in os.listdir(DOCS_DIR):
        if filename.endswith(".txt"):
            filepath = os.path.join(DOCS_DIR, filename)
            with open(filepath, "r", encoding="utf-8") as f:
                text = f.read()
                documents.append(Document(page_content=text, metadata={"source": filename}))
                # íŒŒì¼ ì„ ì½ì–´ LangChain ìš© Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ 

    if not documents:
        raise FileNotFoundError(f"ğŸ“‚ documents í´ë”ì— .txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {DOCS_DIR}")
    
    return documents

# FAISS ë¡œ ë§Œë“  ë²¡í„° ì €ì¥ì†Œë¥´ë¥´ .pkl íŒŒì¼ë¡œ ë¡œì»¬ ì €ì¥. 
def save_vectorstore(vectorstore, path=VECTORSTORE_PATH):
    with open(path, "wb") as f:
        pickle.dump(vectorstore, f)

# def load_vectorstore():
#     docs = load_documents()
#     splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30) # ë¬¸ì„œ ìª¼ê°œê³  ê²¹ì¹˜ë„ë¡ ë‚˜ëˆ” 
#     splits = splitter.split_documents(docs)
#     return FAISS.from_documents(splits, embedding) 
#     # ê°ê°ì˜ split ë¬¸ì„œë¥¼ embedding ëª¨ë¸ë¡œ ìµœì í™” , ë²¡í„°ë¥¼ FAISS ì— ì €ì¥ í›„ vectorstore ê°ì²´ ë°˜í™˜ 

# í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_keywords_by_keybert(text, top_n=3):
    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=top_n)
    return [kw for kw, _ in keywords]

def load_vectorstore():
    if os.path.exists(VECTORSTORE_PATH): # ì´ë¯¸ ë§Œë“¤ì–´ë‘” FAISS ë²¡í„° DB ê°€ ìˆìœ¼ë©´ ë¡œë“œí•´ì„œ ì¬ì‚¬ìš© 
        with open(VECTORSTORE_PATH, "rb") as f:
            print("ìºì‹œëœ vectorstore ë¡œë“œë¨")
            return pickle.load(f)
    else:
        print("vectorstore ìƒˆë¡œ ìƒì„± ì¤‘...")
        docs = load_documents() # ë¬¸ì„œë¡œë”© 
        splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30) # í…ìŠ¤íŠ¸ ë¶„í•  
        splits = splitter.split_documents(docs)
        vectorstore = FAISS.from_documents(splits, embedding) # ë²¡í„°í™” 
        save_vectorstore(vectorstore) # ì €ì¥ 
        return vectorstore

# ë‹µë³€ ìƒì„± í•¨ìˆ˜ 
def generate_rag_answer(model, question, vectorstore, k=4): 

    # í‚¤ì›Œë“œ ì¶”ë£° 
    keywords = extract_keywords_by_keybert(question)
    # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
    query = " ".join(keywords)

    # ì§ˆë¬¸ì— ëŒ€í•´ ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ë¹„ìŠ·í•œ ë¬¸ì„œ kê°œ ê²€ìƒ‰ 
    docs = vectorstore.similarity_search(query, k=k) 

    context = "\n".join([f"ë¬¸ì„œ{i+1}: {doc.page_content}" for i, doc in enumerate(docs)])
    prompt = f"""Contextì— ê¸°ë°˜í•´ì„œ ì§§ê²Œ ëŒ€ë‹µí•´ì¤˜.
    Context:
    {context}
    ì§ˆë¬¸: {question}
    ëŒ€ë‹µ:"""
    response = model.generate_content(prompt)
    answer = response.candidates[0].content.parts[0].text
    return answer, context
